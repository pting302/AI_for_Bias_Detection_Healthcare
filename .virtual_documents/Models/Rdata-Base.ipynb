get_ipython().run_cell_magic("html", "", """<style>
@import url('https://fonts.googleapis.com/css2@family=JetBrains+Mono:wght@300@display=swap');
* {
    font-family: 'JetBrains Mono', monospace;
}
</style>""")


import pyreadr
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import chi2_contingency
import imblearn
from sklearn.preprocessing import LabelEncoder


data_file = '../5v_cleandf.rdata'
r_format = pyreadr.read_r(data_file)
df = r_format['df']  # df is the dataset contained in the file
df.head().T


columns = ['esi',
           'age',
           'gender',
           'ethnicity',
           'race',
           'lang',
           'religion',
           'maritalstatus',
           'employstatus',
           'insurance_status',
           'disposition',
           'arrivalmode',
           'previousdispo']

data = df.copy()[columns] # new dataframe with the selected columns


cols_in_dataframe = list(data.columns)

for col in cols_in_dataframe:
    rows_to_remove = []
    nulled_col = list(data[col].isnull())
    for i in range(len(nulled_col)):
        if nulled_col[i] == True:
            rows_to_remove.append(i)
    rows_to_remove.sort(reverse=True)
    
    data.drop(rows_to_remove, inplace=True)
    data.reset_index(drop=True, inplace=True)
    
    print('Done condensing:', col)


get_ipython().run_cell_magic("capture", "", """cont = ['esi', 'age']
cat = [
    'gender',
    'ethnicity',
    'race',
    'lang',
    'religion',
    'maritalstatus',
    'employstatus',
    'insurance_status',
    'disposition',
    'arrivalmode',
    'previousdispo'
]

# Create individual label encoders for each column
transformers = {}
for col in cat:
    transformers[col] = LabelEncoder()

# fit each column with its corresponding label encoder
for col in cat:
    data[col] = data[[col]].apply(transformers[col].fit_transform)""")


data['esi'] = data['esi'].astype(int)


cols_to_train_on = [
    'esi', 'age', 'gender', 'ethnicity', 'race', 'lang', 'religion',
#     'maritalstatus', 'employstatus', 'insurance_status', 'disposition',
    'maritalstatus', 'employstatus', 'insurance_status',
    'arrivalmode'
]

# col_dep_var = [ 'previousdispo' ]
col_dep_var = [ 'disposition' ]


from sklearn.model_selection import train_test_split


X = data[cols_to_train_on]
y = data[col_dep_var]


X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42
)


train = pd.DataFrame(X_train)
# train['previousdispo'] = y_train
train['disposition'] = y_train
train.head()


test = pd.DataFrame(X_test)
# test['previousdispo'] = y_test
test['disposition'] = y_test
test.head()


# X_train = X_train.drop(columns='previousdispo')
X_train = X_train.drop(columns='disposition')
X_train.head()


# X_test = X_test.drop(columns='previousdispo')
X_test = X_test.drop(columns='disposition')
X_test.head()


from sklearn.neighbors import KNeighborsClassifier # import the module


knn = KNeighborsClassifier() # create the classifier


knn.fit(X_train, np.ravel(y_train)) # fit the classifier


preds = knn.predict(X_test) # predict


knn.score(X_test, y_test)


from sklearn.metrics import classification_report


print(classification_report(y_test, preds))


from sklearn.metrics import f1_score


f1_score(y_test, preds, average='macro')


from sklearn.metrics import confusion_matrix
matrix = confusion_matrix(y_test, preds)
matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(16,7))
sns.set(font_scale=1.4)
sns.heatmap(matrix, annot=True, annot_kws={'size':10},
            cmap=plt.cm.Greens, linewidths=0.2)

# plot_labels = list(data['previousdispo'].unique())
plot_labels = list(data['disposition'].unique())
plot_labels.sort()
# plot_labels_transformed = transformers['previousdispo'].inverse_transform(plot_labels)
plot_labels_transformed = transformers['disposition'].inverse_transform(plot_labels)
class_names = list(plot_labels_transformed)
tick_marks = np.arange(len(class_names)) + 0.5
tick_marks2 = tick_marks + 0.5
plt.xticks(tick_marks, class_names, rotation=90)
plt.yticks(tick_marks2, class_names, rotation=0)
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix for kNN Model')
plt.show()


from sklearn.tree import DecisionTreeClassifier # import the module


clf = DecisionTreeClassifier(random_state=42) # create the classifier


clf.fit(X_train, np.ravel(y_train)) # fit the classifier


preds = clf.predict(X_test) # predict


clf.score(X_test, y_test)


from sklearn.metrics import classification_report


print(classification_report(y_test, preds))


from sklearn.metrics import f1_score


f1_score(y_test, preds, average='macro')


from sklearn.metrics import confusion_matrix
matrix = confusion_matrix(y_test, preds)
matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(16,7))
sns.set(font_scale=1.4)
sns.heatmap(matrix, annot=True, annot_kws={'size':10},
            cmap=plt.cm.Greens, linewidths=0.2)

# plot_labels = list(data['previousdispo'].unique())
plot_labels = list(data['disposition'].unique())
plot_labels.sort()
# plot_labels_transformed = transformers['previousdispo'].inverse_transform(plot_labels)
plot_labels_transformed = transformers['disposition'].inverse_transform(plot_labels)
class_names = list(plot_labels_transformed)
tick_marks = np.arange(len(class_names)) + 0.5
tick_marks2 = tick_marks + 0.5
plt.xticks(tick_marks, class_names, rotation=90)
plt.yticks(tick_marks2, class_names, rotation=0)
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix for Decision Tree Model')
plt.show()


import json


params = dict()
with open('balanced_random_forest_classifier_params.conf', 'r') as fin:
    params = json.load(fin)


params # derived from bayesian optimization


from numpy import mean
from imblearn.ensemble import BalancedRandomForestClassifier # import the module


brc = BalancedRandomForestClassifier(
    max_samples=params['max_samples'],
    max_features=params['max_features'],
    n_estimators=int(params['n_estimators']),
    verbose=True,
    n_jobs=-1,
    random_state=42
) # create the classifier


brc.fit(X_train, np.ravel(y_train)) # fit the classifier


preds = brc.predict(X_test) # predict


brc.score(X_test, y_test)


from sklearn.metrics import classification_report


print(classification_report(y_test, preds))


from sklearn.metrics import f1_score


f1_score(y_test, preds, average='macro')


from sklearn.metrics import confusion_matrix
matrix = confusion_matrix(y_test, preds)
matrix = matrix.astype('float') / matrix.sum(axis=1)[:, np.newaxis]

plt.figure(figsize=(16,7))
sns.set(font_scale=1.4)
sns.heatmap(matrix, annot=True, annot_kws={'size':10},
            cmap=plt.cm.Greens, linewidths=0.2)

# plot_labels = list(data['previousdispo'].unique())
plot_labels = list(data['disposition'].unique())
plot_labels.sort()
# plot_labels_transformed = transformers['previousdispo'].inverse_transform(plot_labels)
plot_labels_transformed = transformers['disposition'].inverse_transform(plot_labels)
class_names = list(plot_labels_transformed)
tick_marks = np.arange(len(class_names)) + 0.5
tick_marks2 = tick_marks + 0.5
plt.xticks(tick_marks, class_names, rotation=90)
plt.yticks(tick_marks2, class_names, rotation=0)
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix for Balanced RandomForest Model')
plt.show()


from sklearn.model_selection import cross_validate


scores = cross_validate(brc, X_train, y_train, cv=5, n_jobs=-1, scoring='f1_macro', verbose=2)
scores


from sklearn.model_selection import cross_val_score


scores = cross_val_score(brc, X_train, y_train, cv=5, n_jobs=-1, scoring='f1_macro', verbose=3)
scores


importances = brc.feature_importances_
importances


std = np.std([tree.feature_importances_ for tree in brc.estimators_], axis=0)
std


feature_names = list(X_train.columns)
feature_names


forest_importances = pd.Series(importances, index=feature_names)


fig, ax = plt.subplots(figsize=(12, 8))
forest_importances.plot.bar(yerr=std, ax=ax)
ax.set_title("Feature importances using MDI")
ax.set_ylabel("Mean decrease in impurity")
fig.tight_layout()


from sklearn.inspection import permutation_importance


result = permutation_importance(
    brc, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1
)


forest_importances = pd.Series(result.importances_mean, index=feature_names)


forest_importances


result.importances_std


fig, ax = plt.subplots(figsize=(12, 8))
forest_importances.plot.bar(yerr=result.importances_std)
ax.set_title("Feature importances using permutation on full model")
ax.set_ylabel("Mean accuracy decrease")
fig.tight_layout()
plt.show()


importances = brc.feature_importances_


indices = np.argsort(importances)


features = X_train.columns


fig = plt.figure(figsize=(12, 8))
plt.title('Feature Importances')
plt.barh(range(len(indices)), importances[indices], color='g', align='center')
plt.yticks(range(len(indices)), [features[i] for i in indices])
plt.xlabel('Relative Importance')
plt.show()


import shap


X_test['age'] = X_test['age'].astype('int')


explainer = shap.Explainer(brc)


shap_values = explainer.shap_values(X_test[:50])


shap.summary_plot(
    shap_values,
    X_train.values,
    class_names= transformers['disposition'].inverse_transform(train['disposition'].unique()),
    feature_names=X_train.columns
)


shap.initjs()
i = 3
shap.force_plot(explainer.expected_value[0], shap_values[0][i], X_train.values[i], feature_names = X_train.columns)


row = 3
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values[0][row], 
        base_values=explainer.expected_value[0],
        data=X_test.iloc[row],
        feature_names=X_test.columns.tolist()
    ),
    max_display=X_test.shape[1]
)
